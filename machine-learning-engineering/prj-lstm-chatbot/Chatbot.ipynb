{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# ChatBot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM/GRU as it's memory unit.\n",
    "\n",
    "The dataset chosen is Squad v2 from `torchtext.datasets`. Check `prepare_data.py` for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code based on Matthew Inkawhich's tutorial <https://github.com/MatthewInkawhich>\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "import os\n",
    "from prepare_data import load_prepare_data, trim_unfrequent_words, batch_to_train_data, get_indexes_from_sentence, normalize_string\n",
    "from models import Seq2Seq, SearchDecoder\n",
    "from globals import SOS_TOKEN, MIN_COUNT, MAX_SENTENCE_LENGTH, device\n",
    "from torchtext.datasets import SQuAD2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Trimmed to 10291 sentence pairs\n",
      "Counted  18213  words in vocab\n",
      "\n",
      "Sample question/answer pairs:\n",
      "['where did beyonce get her name from', 'her mother s maiden name']\n",
      "['what race was beyonce s father', 'african american']\n",
      "['beyonce s mother worked in what industry', 'hairdresser and salon owner']\n",
      "['which of her teachers discovered beyonce s musical talent', 'dance instructor darlette johnson']\n",
      "['what choir did beyonce sing in for two years', 'st . john s united methodist church']\n",
      "['who signed the girl group on october', 'dwayne wiggins s grass roots entertainment']\n",
      "['what event caused beyonce s depression', 'split with luckett and rober']\n",
      "['how long was beyonce depressed', 'a couple of years']\n",
      "['who replaced luckett and roberson in destiny s child', 'farrah franklin and michelle williams .']\n",
      "['who filed a lawsuit over survivor', 'luckett and roberson']\n",
      "keep_words 5382 / 18210 = 0.2956\n",
      "Trimmed from 10291 pairs to 2383, 0.2316 of total\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SQuAD2(split='train')\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"checkpoints\")\n",
    "vocab, pairs = load_prepare_data(train_dataset)\n",
    "\n",
    "# Print first 10 pairs\n",
    "print(\"\\nSample question/answer pairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trim_unfrequent_words(vocab, pairs, MIN_COUNT)\n",
    "\n",
    "# Check sample data with a small batch size\n",
    "small_batch_size = 5\n",
    "batches = batch_to_train_data(vocab, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "# print(\"input_variable:\", input_variable)\n",
    "# print(\"lengths:\", lengths)\n",
    "# print(\"target_variable:\", target_variable)\n",
    "# print(\"mask:\", mask)\n",
    "# print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "We define 3 main classes:\n",
    "- `Encoder`\n",
    "- `Decoder`\n",
    "- `Seq2Seq`\n",
    "Additionally, the `Attention` and `SearchDecoder` classes. And a loss function: `mask_NLLLoss`\n",
    "\n",
    "Check `chatbot/models.py`\n",
    "\n",
    "For both train or evaluate the model, we must initialize encoder and decoder (through Seq2Seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "hidden_size = 1000\n",
    "batch_size = 64\n",
    "\n",
    "# Set checkpoint to load from disk. None to start training from scratch\n",
    "checkpoint_iter = 4000\n",
    "checkpoint_filename = os.path.join(save_dir, '{}'.format(hidden_size), '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "# checkpoint_filename = None      # Comment this to train from scratch, uncomment to load saved checkpoint (trained model)\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if checkpoint_filename:\n",
    "    print('Loading checkpoint from ', checkpoint_filename)\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(checkpoint_filename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['encoder']\n",
    "    decoder_sd = checkpoint['decoder']\n",
    "    encoder_optimizer_sd = checkpoint['encoder_optimizer']\n",
    "    decoder_optimizer_sd = checkpoint['decoder_optimizer']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    vocab.__dict__ = checkpoint['vocab_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(vocab.num_words, hidden_size)\n",
    "if checkpoint_filename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "\n",
    "# Initialize Seq2seq (and encoder & decoder)\n",
    "seq2seq = Seq2Seq(hidden_size, hidden_size, vocab.num_words, embedding)\n",
    "encoder = seq2seq.encoder\n",
    "decoder = seq2seq.decoder\n",
    "\n",
    "if checkpoint_filename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Ready!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We have `train_step` for a single iteration and `train_loop` which runs several epochs. The latter also saves checkpoints every `save_every` iteration in the `checkpoints` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_variable, lengths, target_variable, mask, max_target_len, seq2seq, encoder_optimizer,\n",
    "               decoder_optimizer, batch_size, clip):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss, n_totals, print_losses = seq2seq.forward_batch(input_variable, target_variable, max_target_len, lengths, mask,\n",
    "                                                   teacher_forcing_ratio, batch_size)\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients in place\n",
    "    _ = nn.utils.clip_grad_norm_(seq2seq.encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(seq2seq.decoder.parameters(), clip)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    avg_loss = sum(print_losses) / n_totals\n",
    "    return avg_loss\n",
    "\n",
    "def train_loop(vocab, pairs, seq2seq, encoder_optimizer, decoder_optimizer, embedding, save_dir, epochs,\n",
    "               batch_size, print_every, save_every, clip, checkpoint_filename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch_to_train_data(vocab, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                        for _ in range(epochs)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if checkpoint_filename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for epoch in range(start_iteration, epochs + 1):\n",
    "        training_batch = training_batches[epoch - 1]\n",
    "        # Extract from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train_step(input_variable, lengths, target_variable, mask, max_target_len, seq2seq, encoder_optimizer,\n",
    "                          decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Epoch: {}; Completion: {:.1f}%; Avg loss: {:.4f}\".format(epoch, epoch / epochs * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (epoch % save_every == 0):\n",
    "            directory = os.path.join(save_dir, '{}'.format(hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': epoch,\n",
    "                'vocab_dict': vocab.__dict__,\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'encoder_optimizer': encoder_optimizer.state_dict(),\n",
    "                'decoder': decoder.state_dict(),\n",
    "                'decoder_optimizer': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(epoch, 'checkpoint')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Initializing ...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\edgarin\\mycode\\my-ml-playground\\machine-learning-engineering\\prj-lstm-chatbot\\models.py:154: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:1856.)\n",
      "  loss = cross_entropy.masked_select(mask).mean()\n",
      "C:\\Users\\edgar\\anaconda3\\envs\\dlnd2-env\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:2280.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10; Completion: 0.2%; Avg loss: 8.0014\n",
      "Epoch: 20; Completion: 0.5%; Avg loss: 6.5781\n",
      "Epoch: 30; Completion: 0.8%; Avg loss: 6.2205\n",
      "Epoch: 40; Completion: 1.0%; Avg loss: 6.1914\n",
      "Epoch: 50; Completion: 1.2%; Avg loss: 6.1113\n",
      "Epoch: 60; Completion: 1.5%; Avg loss: 6.0349\n",
      "Epoch: 70; Completion: 1.8%; Avg loss: 5.9107\n",
      "Epoch: 80; Completion: 2.0%; Avg loss: 5.9231\n",
      "Epoch: 90; Completion: 2.2%; Avg loss: 5.9018\n",
      "Epoch: 100; Completion: 2.5%; Avg loss: 5.8078\n",
      "Epoch: 110; Completion: 2.8%; Avg loss: 5.8198\n",
      "Epoch: 120; Completion: 3.0%; Avg loss: 5.8195\n",
      "Epoch: 130; Completion: 3.2%; Avg loss: 5.7150\n",
      "Epoch: 140; Completion: 3.5%; Avg loss: 5.6077\n",
      "Epoch: 150; Completion: 3.8%; Avg loss: 5.6171\n",
      "Epoch: 160; Completion: 4.0%; Avg loss: 5.5671\n",
      "Epoch: 170; Completion: 4.2%; Avg loss: 5.4948\n",
      "Epoch: 180; Completion: 4.5%; Avg loss: 5.3419\n",
      "Epoch: 190; Completion: 4.8%; Avg loss: 5.1994\n",
      "Epoch: 200; Completion: 5.0%; Avg loss: 5.2025\n",
      "Epoch: 210; Completion: 5.2%; Avg loss: 4.9796\n",
      "Epoch: 220; Completion: 5.5%; Avg loss: 5.0095\n",
      "Epoch: 230; Completion: 5.8%; Avg loss: 4.8825\n",
      "Epoch: 240; Completion: 6.0%; Avg loss: 4.7919\n",
      "Epoch: 250; Completion: 6.2%; Avg loss: 4.6495\n",
      "Epoch: 260; Completion: 6.5%; Avg loss: 4.4496\n",
      "Epoch: 270; Completion: 6.8%; Avg loss: 4.3640\n",
      "Epoch: 280; Completion: 7.0%; Avg loss: 4.2722\n",
      "Epoch: 290; Completion: 7.2%; Avg loss: 3.8986\n",
      "Epoch: 300; Completion: 7.5%; Avg loss: 3.9073\n",
      "Epoch: 310; Completion: 7.8%; Avg loss: 3.7127\n",
      "Epoch: 320; Completion: 8.0%; Avg loss: 3.7356\n",
      "Epoch: 330; Completion: 8.2%; Avg loss: 3.5316\n",
      "Epoch: 340; Completion: 8.5%; Avg loss: 3.4043\n",
      "Epoch: 350; Completion: 8.8%; Avg loss: 3.2410\n",
      "Epoch: 360; Completion: 9.0%; Avg loss: 3.0787\n",
      "Epoch: 370; Completion: 9.2%; Avg loss: 2.9576\n",
      "Epoch: 380; Completion: 9.5%; Avg loss: 2.9984\n",
      "Epoch: 390; Completion: 9.8%; Avg loss: 2.7910\n",
      "Epoch: 400; Completion: 10.0%; Avg loss: 2.8567\n",
      "Epoch: 410; Completion: 10.2%; Avg loss: 2.5378\n",
      "Epoch: 420; Completion: 10.5%; Avg loss: 2.4610\n",
      "Epoch: 430; Completion: 10.8%; Avg loss: 2.2772\n",
      "Epoch: 440; Completion: 11.0%; Avg loss: 2.2729\n",
      "Epoch: 450; Completion: 11.2%; Avg loss: 2.2029\n",
      "Epoch: 460; Completion: 11.5%; Avg loss: 2.1005\n",
      "Epoch: 470; Completion: 11.8%; Avg loss: 1.9542\n",
      "Epoch: 480; Completion: 12.0%; Avg loss: 1.9077\n",
      "Epoch: 490; Completion: 12.2%; Avg loss: 1.9095\n",
      "Epoch: 500; Completion: 12.5%; Avg loss: 1.6470\n",
      "Epoch: 510; Completion: 12.8%; Avg loss: 1.6879\n",
      "Epoch: 520; Completion: 13.0%; Avg loss: 1.5571\n",
      "Epoch: 530; Completion: 13.2%; Avg loss: 1.4221\n",
      "Epoch: 540; Completion: 13.5%; Avg loss: 1.3405\n",
      "Epoch: 550; Completion: 13.8%; Avg loss: 1.2857\n",
      "Epoch: 560; Completion: 14.0%; Avg loss: 1.2350\n",
      "Epoch: 570; Completion: 14.2%; Avg loss: 1.1297\n",
      "Epoch: 580; Completion: 14.5%; Avg loss: 1.1335\n",
      "Epoch: 590; Completion: 14.8%; Avg loss: 1.0450\n",
      "Epoch: 600; Completion: 15.0%; Avg loss: 1.0000\n",
      "Epoch: 610; Completion: 15.2%; Avg loss: 1.0278\n",
      "Epoch: 620; Completion: 15.5%; Avg loss: 0.9053\n",
      "Epoch: 630; Completion: 15.8%; Avg loss: 0.8610\n",
      "Epoch: 640; Completion: 16.0%; Avg loss: 0.8868\n",
      "Epoch: 650; Completion: 16.2%; Avg loss: 0.7534\n",
      "Epoch: 660; Completion: 16.5%; Avg loss: 0.7460\n",
      "Epoch: 670; Completion: 16.8%; Avg loss: 0.7338\n",
      "Epoch: 680; Completion: 17.0%; Avg loss: 0.6309\n",
      "Epoch: 690; Completion: 17.2%; Avg loss: 0.6989\n",
      "Epoch: 700; Completion: 17.5%; Avg loss: 0.6220\n",
      "Epoch: 710; Completion: 17.8%; Avg loss: 0.6321\n",
      "Epoch: 720; Completion: 18.0%; Avg loss: 0.6102\n",
      "Epoch: 730; Completion: 18.2%; Avg loss: 0.5701\n",
      "Epoch: 740; Completion: 18.5%; Avg loss: 0.5047\n",
      "Epoch: 750; Completion: 18.8%; Avg loss: 0.4610\n",
      "Epoch: 760; Completion: 19.0%; Avg loss: 0.4567\n",
      "Epoch: 770; Completion: 19.2%; Avg loss: 0.4467\n",
      "Epoch: 780; Completion: 19.5%; Avg loss: 0.4139\n",
      "Epoch: 790; Completion: 19.8%; Avg loss: 0.4092\n",
      "Epoch: 800; Completion: 20.0%; Avg loss: 0.3698\n",
      "Epoch: 810; Completion: 20.2%; Avg loss: 0.3635\n",
      "Epoch: 820; Completion: 20.5%; Avg loss: 0.3218\n",
      "Epoch: 830; Completion: 20.8%; Avg loss: 0.2980\n",
      "Epoch: 840; Completion: 21.0%; Avg loss: 0.2527\n",
      "Epoch: 850; Completion: 21.2%; Avg loss: 0.2604\n",
      "Epoch: 860; Completion: 21.5%; Avg loss: 0.2523\n",
      "Epoch: 870; Completion: 21.8%; Avg loss: 0.2546\n",
      "Epoch: 880; Completion: 22.0%; Avg loss: 0.2287\n",
      "Epoch: 890; Completion: 22.2%; Avg loss: 0.2073\n",
      "Epoch: 900; Completion: 22.5%; Avg loss: 0.2007\n",
      "Epoch: 910; Completion: 22.8%; Avg loss: 0.1811\n",
      "Epoch: 920; Completion: 23.0%; Avg loss: 0.1641\n",
      "Epoch: 930; Completion: 23.2%; Avg loss: 0.1648\n",
      "Epoch: 940; Completion: 23.5%; Avg loss: 0.1621\n",
      "Epoch: 950; Completion: 23.8%; Avg loss: 0.1477\n",
      "Epoch: 960; Completion: 24.0%; Avg loss: 0.1478\n",
      "Epoch: 970; Completion: 24.2%; Avg loss: 0.1464\n",
      "Epoch: 980; Completion: 24.5%; Avg loss: 0.1232\n",
      "Epoch: 990; Completion: 24.8%; Avg loss: 0.1244\n",
      "Epoch: 1000; Completion: 25.0%; Avg loss: 0.1000\n",
      "Epoch: 1010; Completion: 25.2%; Avg loss: 0.0986\n",
      "Epoch: 1020; Completion: 25.5%; Avg loss: 0.0908\n",
      "Epoch: 1030; Completion: 25.8%; Avg loss: 0.1082\n",
      "Epoch: 1040; Completion: 26.0%; Avg loss: 0.0914\n",
      "Epoch: 1050; Completion: 26.2%; Avg loss: 0.0948\n",
      "Epoch: 1060; Completion: 26.5%; Avg loss: 0.0773\n",
      "Epoch: 1070; Completion: 26.8%; Avg loss: 0.0834\n",
      "Epoch: 1080; Completion: 27.0%; Avg loss: 0.0811\n",
      "Epoch: 1090; Completion: 27.3%; Avg loss: 0.0806\n",
      "Epoch: 1100; Completion: 27.5%; Avg loss: 0.0685\n",
      "Epoch: 1110; Completion: 27.8%; Avg loss: 0.0666\n",
      "Epoch: 1120; Completion: 28.0%; Avg loss: 0.0606\n",
      "Epoch: 1130; Completion: 28.2%; Avg loss: 0.0601\n",
      "Epoch: 1140; Completion: 28.5%; Avg loss: 0.0655\n",
      "Epoch: 1150; Completion: 28.7%; Avg loss: 0.0597\n",
      "Epoch: 1160; Completion: 29.0%; Avg loss: 0.0530\n",
      "Epoch: 1170; Completion: 29.2%; Avg loss: 0.0548\n",
      "Epoch: 1180; Completion: 29.5%; Avg loss: 0.0530\n",
      "Epoch: 1190; Completion: 29.8%; Avg loss: 0.0450\n",
      "Epoch: 1200; Completion: 30.0%; Avg loss: 0.0513\n",
      "Epoch: 1210; Completion: 30.2%; Avg loss: 0.0451\n",
      "Epoch: 1220; Completion: 30.5%; Avg loss: 0.0403\n",
      "Epoch: 1230; Completion: 30.8%; Avg loss: 0.0423\n",
      "Epoch: 1240; Completion: 31.0%; Avg loss: 0.0388\n",
      "Epoch: 1250; Completion: 31.2%; Avg loss: 0.0423\n",
      "Epoch: 1260; Completion: 31.5%; Avg loss: 0.0402\n",
      "Epoch: 1270; Completion: 31.8%; Avg loss: 0.0355\n",
      "Epoch: 1280; Completion: 32.0%; Avg loss: 0.0361\n",
      "Epoch: 1290; Completion: 32.2%; Avg loss: 0.0349\n",
      "Epoch: 1300; Completion: 32.5%; Avg loss: 0.0320\n",
      "Epoch: 1310; Completion: 32.8%; Avg loss: 0.0392\n",
      "Epoch: 1320; Completion: 33.0%; Avg loss: 0.0299\n",
      "Epoch: 1330; Completion: 33.2%; Avg loss: 0.0292\n",
      "Epoch: 1340; Completion: 33.5%; Avg loss: 0.0369\n",
      "Epoch: 1350; Completion: 33.8%; Avg loss: 0.0292\n",
      "Epoch: 1360; Completion: 34.0%; Avg loss: 0.0297\n",
      "Epoch: 1370; Completion: 34.2%; Avg loss: 0.0294\n",
      "Epoch: 1380; Completion: 34.5%; Avg loss: 0.0284\n",
      "Epoch: 1390; Completion: 34.8%; Avg loss: 0.0294\n",
      "Epoch: 1400; Completion: 35.0%; Avg loss: 0.0291\n",
      "Epoch: 1410; Completion: 35.2%; Avg loss: 0.0246\n",
      "Epoch: 1420; Completion: 35.5%; Avg loss: 0.0288\n",
      "Epoch: 1430; Completion: 35.8%; Avg loss: 0.0236\n",
      "Epoch: 1440; Completion: 36.0%; Avg loss: 0.0236\n",
      "Epoch: 1450; Completion: 36.2%; Avg loss: 0.0265\n",
      "Epoch: 1460; Completion: 36.5%; Avg loss: 0.0275\n",
      "Epoch: 1470; Completion: 36.8%; Avg loss: 0.0249\n",
      "Epoch: 1480; Completion: 37.0%; Avg loss: 0.0270\n",
      "Epoch: 1490; Completion: 37.2%; Avg loss: 0.0228\n",
      "Epoch: 1500; Completion: 37.5%; Avg loss: 0.0254\n",
      "Epoch: 1510; Completion: 37.8%; Avg loss: 0.0221\n",
      "Epoch: 1520; Completion: 38.0%; Avg loss: 0.0218\n",
      "Epoch: 1530; Completion: 38.2%; Avg loss: 0.0197\n",
      "Epoch: 1540; Completion: 38.5%; Avg loss: 0.0222\n",
      "Epoch: 1550; Completion: 38.8%; Avg loss: 0.0196\n",
      "Epoch: 1560; Completion: 39.0%; Avg loss: 0.0237\n",
      "Epoch: 1570; Completion: 39.2%; Avg loss: 0.0192\n",
      "Epoch: 1580; Completion: 39.5%; Avg loss: 0.0200\n",
      "Epoch: 1590; Completion: 39.8%; Avg loss: 0.0188\n",
      "Epoch: 1600; Completion: 40.0%; Avg loss: 0.0175\n",
      "Epoch: 1610; Completion: 40.2%; Avg loss: 0.0182\n",
      "Epoch: 1620; Completion: 40.5%; Avg loss: 0.0203\n",
      "Epoch: 1630; Completion: 40.8%; Avg loss: 0.0184\n",
      "Epoch: 1640; Completion: 41.0%; Avg loss: 0.0169\n",
      "Epoch: 1650; Completion: 41.2%; Avg loss: 0.0169\n",
      "Epoch: 1660; Completion: 41.5%; Avg loss: 0.0165\n",
      "Epoch: 1670; Completion: 41.8%; Avg loss: 0.0154\n",
      "Epoch: 1680; Completion: 42.0%; Avg loss: 0.0163\n",
      "Epoch: 1690; Completion: 42.2%; Avg loss: 0.0159\n",
      "Epoch: 1700; Completion: 42.5%; Avg loss: 0.0145\n",
      "Epoch: 1710; Completion: 42.8%; Avg loss: 0.0147\n",
      "Epoch: 1720; Completion: 43.0%; Avg loss: 0.0147\n",
      "Epoch: 1730; Completion: 43.2%; Avg loss: 0.0157\n",
      "Epoch: 1740; Completion: 43.5%; Avg loss: 0.0197\n",
      "Epoch: 1750; Completion: 43.8%; Avg loss: 0.0508\n",
      "Epoch: 1760; Completion: 44.0%; Avg loss: 0.0493\n",
      "Epoch: 1770; Completion: 44.2%; Avg loss: 0.0341\n",
      "Epoch: 1780; Completion: 44.5%; Avg loss: 0.0341\n",
      "Epoch: 1790; Completion: 44.8%; Avg loss: 0.0285\n",
      "Epoch: 1800; Completion: 45.0%; Avg loss: 0.0440\n",
      "Epoch: 1810; Completion: 45.2%; Avg loss: 0.0312\n",
      "Epoch: 1820; Completion: 45.5%; Avg loss: 0.0390\n",
      "Epoch: 1830; Completion: 45.8%; Avg loss: 0.0306\n",
      "Epoch: 1840; Completion: 46.0%; Avg loss: 0.0394\n",
      "Epoch: 1850; Completion: 46.2%; Avg loss: 0.0489\n",
      "Epoch: 1860; Completion: 46.5%; Avg loss: 0.0481\n",
      "Epoch: 1870; Completion: 46.8%; Avg loss: 0.0503\n",
      "Epoch: 1880; Completion: 47.0%; Avg loss: 0.0408\n",
      "Epoch: 1890; Completion: 47.2%; Avg loss: 0.0427\n",
      "Epoch: 1900; Completion: 47.5%; Avg loss: 0.0449\n",
      "Epoch: 1910; Completion: 47.8%; Avg loss: 0.0399\n",
      "Epoch: 1920; Completion: 48.0%; Avg loss: 0.0406\n",
      "Epoch: 1930; Completion: 48.2%; Avg loss: 0.0554\n",
      "Epoch: 1940; Completion: 48.5%; Avg loss: 0.0571\n",
      "Epoch: 1950; Completion: 48.8%; Avg loss: 0.0562\n",
      "Epoch: 1960; Completion: 49.0%; Avg loss: 0.0461\n",
      "Epoch: 1970; Completion: 49.2%; Avg loss: 0.0510\n",
      "Epoch: 1980; Completion: 49.5%; Avg loss: 0.0823\n",
      "Epoch: 1990; Completion: 49.8%; Avg loss: 0.0849\n",
      "Epoch: 2000; Completion: 50.0%; Avg loss: 0.0592\n",
      "Epoch: 2010; Completion: 50.2%; Avg loss: 0.0659\n",
      "Epoch: 2020; Completion: 50.5%; Avg loss: 0.0563\n",
      "Epoch: 2030; Completion: 50.7%; Avg loss: 0.0839\n",
      "Epoch: 2040; Completion: 51.0%; Avg loss: 0.0905\n",
      "Epoch: 2050; Completion: 51.2%; Avg loss: 0.0705\n",
      "Epoch: 2060; Completion: 51.5%; Avg loss: 0.0858\n",
      "Epoch: 2070; Completion: 51.7%; Avg loss: 0.0686\n",
      "Epoch: 2080; Completion: 52.0%; Avg loss: 0.0850\n",
      "Epoch: 2090; Completion: 52.2%; Avg loss: 0.0881\n",
      "Epoch: 2100; Completion: 52.5%; Avg loss: 0.1000\n",
      "Epoch: 2110; Completion: 52.8%; Avg loss: 0.0797\n",
      "Epoch: 2120; Completion: 53.0%; Avg loss: 0.0768\n",
      "Epoch: 2130; Completion: 53.2%; Avg loss: 0.0715\n",
      "Epoch: 2140; Completion: 53.5%; Avg loss: 0.0494\n",
      "Epoch: 2150; Completion: 53.8%; Avg loss: 0.0597\n",
      "Epoch: 2160; Completion: 54.0%; Avg loss: 0.0497\n",
      "Epoch: 2170; Completion: 54.2%; Avg loss: 0.0541\n",
      "Epoch: 2180; Completion: 54.5%; Avg loss: 0.0389\n",
      "Epoch: 2190; Completion: 54.8%; Avg loss: 0.0527\n",
      "Epoch: 2200; Completion: 55.0%; Avg loss: 0.0365\n",
      "Epoch: 2210; Completion: 55.2%; Avg loss: 0.0515\n",
      "Epoch: 2220; Completion: 55.5%; Avg loss: 0.0387\n",
      "Epoch: 2230; Completion: 55.8%; Avg loss: 0.0299\n",
      "Epoch: 2240; Completion: 56.0%; Avg loss: 0.0255\n",
      "Epoch: 2250; Completion: 56.2%; Avg loss: 0.0421\n",
      "Epoch: 2260; Completion: 56.5%; Avg loss: 0.0445\n",
      "Epoch: 2270; Completion: 56.8%; Avg loss: 0.0348\n",
      "Epoch: 2280; Completion: 57.0%; Avg loss: 0.0298\n",
      "Epoch: 2290; Completion: 57.2%; Avg loss: 0.0427\n",
      "Epoch: 2300; Completion: 57.5%; Avg loss: 0.0328\n",
      "Epoch: 2310; Completion: 57.8%; Avg loss: 0.0274\n",
      "Epoch: 2320; Completion: 58.0%; Avg loss: 0.0245\n",
      "Epoch: 2330; Completion: 58.2%; Avg loss: 0.0215\n",
      "Epoch: 2340; Completion: 58.5%; Avg loss: 0.0294\n",
      "Epoch: 2350; Completion: 58.8%; Avg loss: 0.0256\n",
      "Epoch: 2360; Completion: 59.0%; Avg loss: 0.0180\n",
      "Epoch: 2370; Completion: 59.2%; Avg loss: 0.0153\n",
      "Epoch: 2380; Completion: 59.5%; Avg loss: 0.0257\n",
      "Epoch: 2390; Completion: 59.8%; Avg loss: 0.0184\n",
      "Epoch: 2400; Completion: 60.0%; Avg loss: 0.0128\n",
      "Epoch: 2410; Completion: 60.2%; Avg loss: 0.0183\n",
      "Epoch: 2420; Completion: 60.5%; Avg loss: 0.0171\n",
      "Epoch: 2430; Completion: 60.8%; Avg loss: 0.0174\n",
      "Epoch: 2440; Completion: 61.0%; Avg loss: 0.0110\n",
      "Epoch: 2450; Completion: 61.3%; Avg loss: 0.0131\n",
      "Epoch: 2460; Completion: 61.5%; Avg loss: 0.0094\n",
      "Epoch: 2470; Completion: 61.8%; Avg loss: 0.0094\n",
      "Epoch: 2480; Completion: 62.0%; Avg loss: 0.0112\n",
      "Epoch: 2490; Completion: 62.3%; Avg loss: 0.0082\n",
      "Epoch: 2500; Completion: 62.5%; Avg loss: 0.0078\n",
      "Epoch: 2510; Completion: 62.7%; Avg loss: 0.0098\n",
      "Epoch: 2520; Completion: 63.0%; Avg loss: 0.0122\n",
      "Epoch: 2530; Completion: 63.2%; Avg loss: 0.0100\n",
      "Epoch: 2540; Completion: 63.5%; Avg loss: 0.0096\n",
      "Epoch: 2550; Completion: 63.7%; Avg loss: 0.0091\n",
      "Epoch: 2560; Completion: 64.0%; Avg loss: 0.0078\n",
      "Epoch: 2570; Completion: 64.2%; Avg loss: 0.0072\n",
      "Epoch: 2580; Completion: 64.5%; Avg loss: 0.0063\n",
      "Epoch: 2590; Completion: 64.8%; Avg loss: 0.0062\n",
      "Epoch: 2600; Completion: 65.0%; Avg loss: 0.0062\n",
      "Epoch: 2610; Completion: 65.2%; Avg loss: 0.0108\n",
      "Epoch: 2620; Completion: 65.5%; Avg loss: 0.0061\n",
      "Epoch: 2630; Completion: 65.8%; Avg loss: 0.0094\n",
      "Epoch: 2640; Completion: 66.0%; Avg loss: 0.0062\n",
      "Epoch: 2650; Completion: 66.2%; Avg loss: 0.0068\n",
      "Epoch: 2660; Completion: 66.5%; Avg loss: 0.0081\n",
      "Epoch: 2670; Completion: 66.8%; Avg loss: 0.0061\n",
      "Epoch: 2680; Completion: 67.0%; Avg loss: 0.0065\n",
      "Epoch: 2690; Completion: 67.2%; Avg loss: 0.0075\n",
      "Epoch: 2700; Completion: 67.5%; Avg loss: 0.0069\n",
      "Epoch: 2710; Completion: 67.8%; Avg loss: 0.0066\n",
      "Epoch: 2720; Completion: 68.0%; Avg loss: 0.0051\n",
      "Epoch: 2730; Completion: 68.2%; Avg loss: 0.0057\n",
      "Epoch: 2740; Completion: 68.5%; Avg loss: 0.0057\n",
      "Epoch: 2750; Completion: 68.8%; Avg loss: 0.0050\n",
      "Epoch: 2760; Completion: 69.0%; Avg loss: 0.0057\n",
      "Epoch: 2770; Completion: 69.2%; Avg loss: 0.0065\n",
      "Epoch: 2780; Completion: 69.5%; Avg loss: 0.0092\n",
      "Epoch: 2790; Completion: 69.8%; Avg loss: 0.0051\n",
      "Epoch: 2800; Completion: 70.0%; Avg loss: 0.0044\n",
      "Epoch: 2810; Completion: 70.2%; Avg loss: 0.0076\n",
      "Epoch: 2820; Completion: 70.5%; Avg loss: 0.0043\n",
      "Epoch: 2830; Completion: 70.8%; Avg loss: 0.0047\n",
      "Epoch: 2840; Completion: 71.0%; Avg loss: 0.0044\n",
      "Epoch: 2850; Completion: 71.2%; Avg loss: 0.0056\n",
      "Epoch: 2860; Completion: 71.5%; Avg loss: 0.0061\n",
      "Epoch: 2870; Completion: 71.8%; Avg loss: 0.0052\n",
      "Epoch: 2880; Completion: 72.0%; Avg loss: 0.0042\n",
      "Epoch: 2890; Completion: 72.2%; Avg loss: 0.0044\n",
      "Epoch: 2900; Completion: 72.5%; Avg loss: 0.0045\n",
      "Epoch: 2910; Completion: 72.8%; Avg loss: 0.0041\n",
      "Epoch: 2920; Completion: 73.0%; Avg loss: 0.0042\n",
      "Epoch: 2930; Completion: 73.2%; Avg loss: 0.0040\n",
      "Epoch: 2940; Completion: 73.5%; Avg loss: 0.0065\n",
      "Epoch: 2950; Completion: 73.8%; Avg loss: 0.0042\n",
      "Epoch: 2960; Completion: 74.0%; Avg loss: 0.0075\n",
      "Epoch: 2970; Completion: 74.2%; Avg loss: 0.0057\n",
      "Epoch: 2980; Completion: 74.5%; Avg loss: 0.0039\n",
      "Epoch: 2990; Completion: 74.8%; Avg loss: 0.0037\n",
      "Epoch: 3000; Completion: 75.0%; Avg loss: 0.0040\n",
      "Epoch: 3010; Completion: 75.2%; Avg loss: 0.0090\n",
      "Epoch: 3020; Completion: 75.5%; Avg loss: 0.0040\n",
      "Epoch: 3030; Completion: 75.8%; Avg loss: 0.0034\n",
      "Epoch: 3040; Completion: 76.0%; Avg loss: 0.0068\n",
      "Epoch: 3050; Completion: 76.2%; Avg loss: 0.0049\n",
      "Epoch: 3060; Completion: 76.5%; Avg loss: 0.0046\n",
      "Epoch: 3070; Completion: 76.8%; Avg loss: 0.0056\n",
      "Epoch: 3080; Completion: 77.0%; Avg loss: 0.0042\n",
      "Epoch: 3090; Completion: 77.2%; Avg loss: 0.0039\n",
      "Epoch: 3100; Completion: 77.5%; Avg loss: 0.0091\n",
      "Epoch: 3110; Completion: 77.8%; Avg loss: 0.0044\n",
      "Epoch: 3120; Completion: 78.0%; Avg loss: 0.0047\n",
      "Epoch: 3130; Completion: 78.2%; Avg loss: 0.0032\n",
      "Epoch: 3140; Completion: 78.5%; Avg loss: 0.0031\n",
      "Epoch: 3150; Completion: 78.8%; Avg loss: 0.0032\n",
      "Epoch: 3160; Completion: 79.0%; Avg loss: 0.0045\n",
      "Epoch: 3170; Completion: 79.2%; Avg loss: 0.0032\n",
      "Epoch: 3180; Completion: 79.5%; Avg loss: 0.0057\n",
      "Epoch: 3190; Completion: 79.8%; Avg loss: 0.0050\n",
      "Epoch: 3200; Completion: 80.0%; Avg loss: 0.0043\n",
      "Epoch: 3210; Completion: 80.2%; Avg loss: 0.0049\n",
      "Epoch: 3220; Completion: 80.5%; Avg loss: 0.0038\n",
      "Epoch: 3230; Completion: 80.8%; Avg loss: 0.0051\n",
      "Epoch: 3240; Completion: 81.0%; Avg loss: 0.0032\n",
      "Epoch: 3250; Completion: 81.2%; Avg loss: 0.0034\n",
      "Epoch: 3260; Completion: 81.5%; Avg loss: 0.0033\n",
      "Epoch: 3270; Completion: 81.8%; Avg loss: 0.0042\n",
      "Epoch: 3280; Completion: 82.0%; Avg loss: 0.0030\n",
      "Epoch: 3290; Completion: 82.2%; Avg loss: 0.0049\n",
      "Epoch: 3300; Completion: 82.5%; Avg loss: 0.0029\n",
      "Epoch: 3310; Completion: 82.8%; Avg loss: 0.0041\n",
      "Epoch: 3320; Completion: 83.0%; Avg loss: 0.0056\n",
      "Epoch: 3330; Completion: 83.2%; Avg loss: 0.0028\n",
      "Epoch: 3340; Completion: 83.5%; Avg loss: 0.0041\n",
      "Epoch: 3350; Completion: 83.8%; Avg loss: 0.0047\n",
      "Epoch: 3360; Completion: 84.0%; Avg loss: 0.0029\n",
      "Epoch: 3370; Completion: 84.2%; Avg loss: 0.0027\n",
      "Epoch: 3380; Completion: 84.5%; Avg loss: 0.0030\n",
      "Epoch: 3390; Completion: 84.8%; Avg loss: 0.0027\n",
      "Epoch: 3400; Completion: 85.0%; Avg loss: 0.0037\n",
      "Epoch: 3410; Completion: 85.2%; Avg loss: 0.0025\n",
      "Epoch: 3420; Completion: 85.5%; Avg loss: 0.0025\n",
      "Epoch: 3430; Completion: 85.8%; Avg loss: 0.0025\n",
      "Epoch: 3440; Completion: 86.0%; Avg loss: 0.0025\n",
      "Epoch: 3450; Completion: 86.2%; Avg loss: 0.0037\n",
      "Epoch: 3460; Completion: 86.5%; Avg loss: 0.0039\n",
      "Epoch: 3470; Completion: 86.8%; Avg loss: 0.0024\n",
      "Epoch: 3480; Completion: 87.0%; Avg loss: 0.0023\n",
      "Epoch: 3490; Completion: 87.2%; Avg loss: 0.0058\n",
      "Epoch: 3500; Completion: 87.5%; Avg loss: 0.0027\n",
      "Epoch: 3510; Completion: 87.8%; Avg loss: 0.0023\n",
      "Epoch: 3520; Completion: 88.0%; Avg loss: 0.0024\n",
      "Epoch: 3530; Completion: 88.2%; Avg loss: 0.0023\n",
      "Epoch: 3540; Completion: 88.5%; Avg loss: 0.0027\n",
      "Epoch: 3550; Completion: 88.8%; Avg loss: 0.0070\n",
      "Epoch: 3560; Completion: 89.0%; Avg loss: 0.0024\n",
      "Epoch: 3570; Completion: 89.2%; Avg loss: 0.0038\n",
      "Epoch: 3580; Completion: 89.5%; Avg loss: 0.0029\n",
      "Epoch: 3590; Completion: 89.8%; Avg loss: 0.0055\n",
      "Epoch: 3600; Completion: 90.0%; Avg loss: 0.0039\n",
      "Epoch: 3610; Completion: 90.2%; Avg loss: 0.0034\n",
      "Epoch: 3620; Completion: 90.5%; Avg loss: 0.0041\n",
      "Epoch: 3630; Completion: 90.8%; Avg loss: 0.0034\n",
      "Epoch: 3640; Completion: 91.0%; Avg loss: 0.0030\n",
      "Epoch: 3650; Completion: 91.2%; Avg loss: 0.0037\n",
      "Epoch: 3660; Completion: 91.5%; Avg loss: 0.0035\n",
      "Epoch: 3670; Completion: 91.8%; Avg loss: 0.0049\n",
      "Epoch: 3680; Completion: 92.0%; Avg loss: 0.0032\n",
      "Epoch: 3690; Completion: 92.2%; Avg loss: 0.0037\n",
      "Epoch: 3700; Completion: 92.5%; Avg loss: 0.0025\n",
      "Epoch: 3710; Completion: 92.8%; Avg loss: 0.0031\n",
      "Epoch: 3720; Completion: 93.0%; Avg loss: 0.0020\n",
      "Epoch: 3730; Completion: 93.2%; Avg loss: 0.0038\n",
      "Epoch: 3740; Completion: 93.5%; Avg loss: 0.0023\n",
      "Epoch: 3750; Completion: 93.8%; Avg loss: 0.0027\n",
      "Epoch: 3760; Completion: 94.0%; Avg loss: 0.0042\n",
      "Epoch: 3770; Completion: 94.2%; Avg loss: 0.0032\n",
      "Epoch: 3780; Completion: 94.5%; Avg loss: 0.0055\n",
      "Epoch: 3790; Completion: 94.8%; Avg loss: 0.0019\n",
      "Epoch: 3800; Completion: 95.0%; Avg loss: 0.0019\n",
      "Epoch: 3810; Completion: 95.2%; Avg loss: 0.0050\n",
      "Epoch: 3820; Completion: 95.5%; Avg loss: 0.0060\n",
      "Epoch: 3830; Completion: 95.8%; Avg loss: 0.0018\n",
      "Epoch: 3840; Completion: 96.0%; Avg loss: 0.0032\n",
      "Epoch: 3850; Completion: 96.2%; Avg loss: 0.0018\n",
      "Epoch: 3860; Completion: 96.5%; Avg loss: 0.0046\n",
      "Epoch: 3870; Completion: 96.8%; Avg loss: 0.0036\n",
      "Epoch: 3880; Completion: 97.0%; Avg loss: 0.0032\n",
      "Epoch: 3890; Completion: 97.2%; Avg loss: 0.0037\n",
      "Epoch: 3900; Completion: 97.5%; Avg loss: 0.0017\n",
      "Epoch: 3910; Completion: 97.8%; Avg loss: 0.0017\n",
      "Epoch: 3920; Completion: 98.0%; Avg loss: 0.0017\n",
      "Epoch: 3930; Completion: 98.2%; Avg loss: 0.0023\n",
      "Epoch: 3940; Completion: 98.5%; Avg loss: 0.0019\n",
      "Epoch: 3950; Completion: 98.8%; Avg loss: 0.0018\n",
      "Epoch: 3960; Completion: 99.0%; Avg loss: 0.0019\n",
      "Epoch: 3970; Completion: 99.2%; Avg loss: 0.0037\n",
      "Epoch: 3980; Completion: 99.5%; Avg loss: 0.0021\n",
      "Epoch: 3990; Completion: 99.8%; Avg loss: 0.0018\n",
      "Epoch: 4000; Completion: 100.0%; Avg loss: 0.0049\n"
     ]
    }
   ],
   "source": [
    "# Configure training/optimization hyperparams.\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "epochs = 4000\n",
    "\n",
    "print_every = 10\n",
    "save_every = 500\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Instantiate ptimizers\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if checkpoint_filename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Start training...\")\n",
    "train_loop(vocab, pairs, seq2seq, encoder_optimizer, decoder_optimizer, embedding, save_dir, epochs, batch_size,\n",
    "           print_every, save_every, clip, checkpoint_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manages the low-level process of handling the input sentence\n",
    "def evaluate(searcher, vocab, sentence, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
    "    # words to indexes\n",
    "    indexes_batch = [get_indexes_from_sentence(vocab, sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_sentence_length)\n",
    "    # indexes to words\n",
    "    decoded_words = [vocab.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def eval_input(searcher, vocab):\n",
    "    while(True):\n",
    "        try:\n",
    "            input_sentence = input('You> ')\n",
    "            print('You> ', input_sentence, flush=True)\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            output_words = evaluate(searcher, vocab, input_sentence)\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Chatbot>', ' '.join(output_words), flush=True)\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"ERROR: Unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You>  where do you live\n",
      "Chatbot> southern asian asia\n",
      "You>  what do you like in asia\n",
      "Chatbot> interactions with other resistance\n",
      "You>  I also do politics\n",
      "Chatbot> vinyl in some bc\n",
      "You>  vinyl? are you speaking in code?\n",
      "Chatbot> gather are some metals\n",
      "You>  Digging gold in Asia I guess\n",
      "ERROR: Unknown word.\n",
      "You>  tell me more about that business\n",
      "Chatbot> middle of the first dense\n",
      "You>  ok, please let's talk by phone\n",
      "ERROR: Unknown word.\n",
      "You>  q\n"
     ]
    }
   ],
   "source": [
    "# Run Evaluation!\n",
    "# Set models to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Instantiate search module\n",
    "searcher = SearchDecoder(seq2seq)\n",
    "\n",
    "# Begin chatting with Chatbot\n",
    "eval_input(searcher, vocab)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
