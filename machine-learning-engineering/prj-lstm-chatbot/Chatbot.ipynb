{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM/GRU as it's memory unit.\n",
    "\n",
    "---\n",
    "Code based on Matthew Inkawhich's <https://github.com/MatthewInkawhich>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Read 11873 sentence pairs\n",
      "Trimmed to 872 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 3389\n",
      "\n",
      "pairs:\n",
      "['were the normans in normandy', 'th and th centuries']\n",
      "['from countries did the norse originate', 'denmark iceland and norway']\n",
      "['was the duke in the battle of hastings', 'william the conqueror']\n",
      "['was dyrrachium located', 'the adriatic']\n",
      "['did emma marry', 'king ethelred ii']\n",
      "['was emma s brother', 'duke richard ii']\n",
      "['kicked ethelred out', 'sweyn forkbeard']\n",
      "['did harold ii die', 'battle of hastings']\n",
      "['killed harold ii', 'william ii']\n",
      "['was margaret s husband', 'king malcolm iii of scotland']\n",
      "keep_words 642 / 3386 = 0.1896\n",
      "Trimmed from 872 pairs to 25, 0.0287 of total\n",
      "input_variable: tensor([[229, 252,  30, 118,  72],\n",
      "        [ 15,  46,   4,  10,  15],\n",
      "        [183,   4, 136, 513, 422],\n",
      "        [106, 251, 243, 314,  30],\n",
      "        [ 12, 252,  15,   5, 423],\n",
      "        [ 22,   5, 244,   2,   2],\n",
      "        [ 62,   4, 168,   0,   0],\n",
      "        [  4, 161,   2,   0,   0],\n",
      "        [394, 240,   0,   0,   0],\n",
      "        [  2,   2,   0,   0,   0]])\n",
      "lengths: tensor([10, 10,  8,  6,  6])\n",
      "target_variable: tensor([[158,   4, 242, 514, 424],\n",
      "        [ 15, 161, 245,  15, 422],\n",
      "        [183, 252,   4, 237,   2],\n",
      "        [ 20,  15, 161, 470,   0],\n",
      "        [106, 253, 240,   2,   0],\n",
      "        [ 15,   2,   2,   0,   0],\n",
      "        [151,   0,   0,   0,   0],\n",
      "        [  2,   0,   0,   0,   0]])\n",
      "mask: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "max_target_len: 8\n",
      "Building encoder and decoder ...\n",
      "Ready!\n",
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\edgarin\\mycode\\my-ml-playground\\machine-learning-engineering\\prj-lstm-chatbot\\chatbot\\models.py:121: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:1856.)\n",
      "  loss = crossEntropy.masked_select(mask).mean()\n",
      "C:\\Users\\edgar\\anaconda3\\envs\\dlnd2-env\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:2280.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Completion: 2.5%; Avg loss: 6.4737\n",
      "Epoch: 2; Completion: 5.0%; Avg loss: 6.3452\n",
      "Epoch: 3; Completion: 7.5%; Avg loss: 6.2201\n",
      "Epoch: 4; Completion: 10.0%; Avg loss: 5.9907\n",
      "Epoch: 5; Completion: 12.5%; Avg loss: 5.7518\n",
      "Epoch: 6; Completion: 15.0%; Avg loss: 5.2150\n",
      "Epoch: 7; Completion: 17.5%; Avg loss: 5.0415\n",
      "Epoch: 8; Completion: 20.0%; Avg loss: 5.0504\n",
      "Epoch: 9; Completion: 22.5%; Avg loss: 4.7782\n",
      "Epoch: 10; Completion: 25.0%; Avg loss: 4.6882\n",
      "Epoch: 11; Completion: 27.5%; Avg loss: 4.1717\n",
      "Epoch: 12; Completion: 30.0%; Avg loss: 4.0441\n",
      "Epoch: 13; Completion: 32.5%; Avg loss: 3.7499\n",
      "Epoch: 14; Completion: 35.0%; Avg loss: 3.7443\n",
      "Epoch: 15; Completion: 37.5%; Avg loss: 3.6436\n",
      "Epoch: 16; Completion: 40.0%; Avg loss: 3.5096\n",
      "Epoch: 17; Completion: 42.5%; Avg loss: 3.4072\n",
      "Epoch: 18; Completion: 45.0%; Avg loss: 3.4090\n",
      "Epoch: 19; Completion: 47.5%; Avg loss: 3.1824\n",
      "Epoch: 20; Completion: 50.0%; Avg loss: 3.2454\n",
      "Epoch: 21; Completion: 52.5%; Avg loss: 3.3352\n",
      "Epoch: 22; Completion: 55.0%; Avg loss: 3.0601\n",
      "Epoch: 23; Completion: 57.5%; Avg loss: 3.0667\n",
      "Epoch: 24; Completion: 60.0%; Avg loss: 2.9619\n",
      "Epoch: 25; Completion: 62.5%; Avg loss: 3.0121\n",
      "Epoch: 26; Completion: 65.0%; Avg loss: 2.8378\n",
      "Epoch: 27; Completion: 67.5%; Avg loss: 2.9962\n",
      "Epoch: 28; Completion: 70.0%; Avg loss: 2.8319\n",
      "Epoch: 29; Completion: 72.5%; Avg loss: 2.7343\n",
      "Epoch: 30; Completion: 75.0%; Avg loss: 2.6626\n",
      "Epoch: 31; Completion: 77.5%; Avg loss: 2.7058\n",
      "Epoch: 32; Completion: 80.0%; Avg loss: 2.6263\n",
      "Epoch: 33; Completion: 82.5%; Avg loss: 2.5289\n",
      "Epoch: 34; Completion: 85.0%; Avg loss: 2.4738\n",
      "Epoch: 35; Completion: 87.5%; Avg loss: 2.4276\n",
      "Epoch: 36; Completion: 90.0%; Avg loss: 2.4240\n",
      "Epoch: 37; Completion: 92.5%; Avg loss: 2.4046\n",
      "Epoch: 38; Completion: 95.0%; Avg loss: 2.2163\n",
      "Epoch: 39; Completion: 97.5%; Avg loss: 2.2242\n",
      "Epoch: 40; Completion: 100.0%; Avg loss: 2.2472\n",
      "Chatbot> the of\n",
      "Chatbot> the of\n",
      "Chatbot> the of\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Chatbot Tutorial\n",
    "================\n",
    "Code based on Matthew Inkawhich's <https://github.com/MatthewInkawhich>\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "import os\n",
    "from chatbot.prepare_data import load_prepare_data, train_dataset, trim_unfrequent_words, batch_to_train_data, MAX_SENTENCE_LENGTH, get_indexes_from_sentence, normalize_string\n",
    "from chatbot.models import Seq2Seq\n",
    "from chatbot.globals import SOS_TOKEN, device\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"checkpoints\")\n",
    "vocab, pairs = load_prepare_data(train_dataset)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)\n",
    "\n",
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trim_unfrequent_words(vocab, pairs, MIN_COUNT)\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch_to_train_data(vocab, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)\n",
    "\n",
    "######################################################################\n",
    "# Training\n",
    "\n",
    "def train_step(input_variable, lengths, target_variable, mask, max_target_len, seq2seq, encoder_optimizer,\n",
    "               decoder_optimizer, batch_size, clip):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss, n_totals, print_losses = seq2seq.forward_batch(input_variable, target_variable, max_target_len, lengths, mask,\n",
    "                                                   teacher_forcing_ratio, batch_size)\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(seq2seq.encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(seq2seq.decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    avg_loss = sum(print_losses) / n_totals\n",
    "    return avg_loss\n",
    "\n",
    "def train_loop(vocab, pairs, seq2seq, encoder_optimizer, decoder_optimizer, embedding, save_dir, epochs,\n",
    "               batch_size, print_every, save_every, clip, load_filename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch_to_train_data(vocab, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                        for _ in range(epochs)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if load_filename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for epoch in range(start_iteration, epochs + 1):\n",
    "        training_batch = training_batches[epoch - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train_step(input_variable, lengths, target_variable, mask, max_target_len, seq2seq, encoder_optimizer,\n",
    "                          decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Epoch: {}; Completion: {:.1f}%; Avg loss: {:.4f}\".format(epoch, epoch / epochs * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (epoch % save_every == 0):\n",
    "            directory = os.path.join(save_dir, '{}'.format(hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': epoch,\n",
    "                'vocab_dict': vocab.__dict__,\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'encoder_optimizer': encoder_optimizer.state_dict(),\n",
    "                'decoder': decoder.state_dict(),\n",
    "                'decoder_optimizer': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(epoch, 'checkpoint')))\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Define Evaluation\n",
    "# -----------------\n",
    "#\n",
    "# After training a model, we want to be able to talk to the bot ourselves.\n",
    "# First, we must define how we want the model to decode the encoded input.\n",
    "#\n",
    "# Greedy decoding\n",
    "# ~~~~~~~~~~~~~~~\n",
    "#\n",
    "# Greedy decoding is the decoding method that we use during training when\n",
    "# we are **NOT** using teacher forcing. In other words, for each time\n",
    "# step, we simply choose the word from ``decoder_output`` with the highest\n",
    "# softmax value. This decoding method is optimal on a single time-step\n",
    "# level.\n",
    "#\n",
    "# To facilite the greedy decoding operation, we define a\n",
    "# ``GreedySearchDecoder`` class. When run, an object of this class takes\n",
    "# an input sequence (``input_seq``) of shape *(input_seq length, 1)*, a\n",
    "# scalar input length (``input_length``) tensor, and a ``max_length`` to\n",
    "# bound the response sentence length. The input sentence is evaluated\n",
    "# using the following computational graph:\n",
    "#\n",
    "# **Computation Graph:**\n",
    "#\n",
    "#    1) Forward input through encoder model.\n",
    "#    2) Prepare encoder's final hidden layer to be first hidden input to the decoder.\n",
    "#    3) Initialize decoder's first input as SOS_token.\n",
    "#    4) Initialize tensors to append decoded words to.\n",
    "#    5) Iteratively decode one word token at a time:\n",
    "#        a) Forward pass through decoder.\n",
    "#        b) Obtain most likely word token and its softmax score.\n",
    "#        c) Record token and score.\n",
    "#        d) Prepare current token to be next decoder input.\n",
    "#    6) Return collections of word tokens and scores.\n",
    "#\n",
    "\n",
    "class SearchDecoder(nn.Module):\n",
    "    def __init__(self, seq2seq):\n",
    "        super(SearchDecoder, self).__init__()\n",
    "        self.seq2seq = seq2seq\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.seq2seq.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.num_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_TOKEN\n",
    "        # Initialize tensors to append decoded words to\n",
    "        tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.seq2seq.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            tokens = torch.cat((tokens, decoder_input), dim=0)\n",
    "            scores = torch.cat((scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return tokens, scores\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Evaluate my text\n",
    "# ~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# Now that we have our decoding method defined, we can write functions for\n",
    "# evaluating a string input sentence. The ``evaluate`` function manages\n",
    "# the low-level process of handling the input sentence. We first format\n",
    "# the sentence as an input batch of word indexes with *batch_size==1*. We\n",
    "# do this by converting the words of the sentence to their corresponding\n",
    "# indexes, and transposing the dimensions to prepare the tensor for our\n",
    "# models. We also create a ``lengths`` tensor which contains the length of\n",
    "# our input sentence. In this case, ``lengths`` is scalar because we are\n",
    "# only evaluating one sentence at a time (batch_size==1). Next, we obtain\n",
    "# the decoded response sentence tensor using our ``GreedySearchDecoder``\n",
    "# object (``searcher``). Finally, we convert the response’s indexes to\n",
    "# words and return the list of decoded words.\n",
    "#\n",
    "# ``evaluateInput`` acts as the user interface for our chatbot. When\n",
    "# called, an input text field will spawn in which we can enter our query\n",
    "# sentence. After typing our input sentence and pressing *Enter*, our text\n",
    "# is normalized in the same way as our training data, and is ultimately\n",
    "# fed to the ``evaluate`` function to obtain a decoded output sentence. We\n",
    "# loop this process, so we can keep chatting with our bot until we enter\n",
    "# either “q” or “quit”.\n",
    "#\n",
    "# Finally, if a sentence is entered that contains a word that is not in\n",
    "# the vocabulary, we handle this gracefully by printing an error message\n",
    "# and prompting the user to enter another sentence.\n",
    "#\n",
    "\n",
    "def evaluate(searcher, vocab, sentence, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [get_indexes_from_sentence(vocab, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_sentence_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [vocab.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def eval_input(searcher, vocab):\n",
    "    while(True):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('You> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(searcher, vocab, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Chatbot>', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"ERROR: Unknown word.\")\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Run Model\n",
    "# ---------\n",
    "#\n",
    "# Finally, it is time to run our model!\n",
    "#\n",
    "# Regardless of whether we want to train or test the chatbot model, we\n",
    "# must initialize the individual encoder and decoder models. In the\n",
    "# following block, we set our desired configurations, choose to start from\n",
    "# scratch or set a checkpoint to load from, and build and initialize the\n",
    "# models. Feel free to play with different model configurations to\n",
    "# optimize performance.\n",
    "#\n",
    "\n",
    "# Configure models\n",
    "hidden_size = 500\n",
    "batch_size = 64\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "\n",
    "checkpoint_iter = 4000\n",
    "load_filename = os.path.join(save_dir, '{}'.format(hidden_size), '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "# load_filename = None      # Comment this to train from scratch, uncomment to load saved checkpoint\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if load_filename:\n",
    "    print('Loading from ', load_filename)\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(load_filename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['encoder']\n",
    "    decoder_sd = checkpoint['decoder']\n",
    "    encoder_optimizer_sd = checkpoint['encoder_optimizer']\n",
    "    decoder_optimizer_sd = checkpoint['decoder_optimizer']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    vocab.__dict__ = checkpoint['vocab_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(vocab.num_words, hidden_size)\n",
    "if load_filename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "seq2seq = Seq2Seq(hidden_size, hidden_size, vocab.num_words, embedding)\n",
    "encoder = seq2seq.encoder\n",
    "decoder = seq2seq.decoder\n",
    "if load_filename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Ready!')\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Run Training\n",
    "# ~~~~~~~~~~~~\n",
    "#\n",
    "# Run the following block if you want to train the model.\n",
    "#\n",
    "# First we set training parameters, then we initialize our optimizers, and\n",
    "# finally we call the ``trainIters`` function to run our training\n",
    "# iterations.\n",
    "#\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "epochs = 4000\n",
    "\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if load_filename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "train_loop(vocab, pairs, seq2seq, encoder_optimizer, decoder_optimizer, embedding, save_dir, epochs, batch_size,\n",
    "           print_every, save_every, clip, load_filename)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Run Evaluation\n",
    "# ~~~~~~~~~~~~~~\n",
    "#\n",
    "# To chat with your model, run the following block.\n",
    "#\n",
    "\n",
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = SearchDecoder(seq2seq)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "eval_input(searcher, vocab)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Conclusion\n",
    "# ----------\n",
    "#\n",
    "# That’s all for this one, folks. Congratulations, you now know the\n",
    "# fundamentals to building a generative chatbot model! If you’re\n",
    "# interested, you can try tailoring the chatbot’s behavior by tweaking the\n",
    "# model and training parameters and customizing the data that you train\n",
    "# the model on.\n",
    "#\n",
    "# Check out the other tutorials for more cool deep learning applications\n",
    "# in PyTorch!\n",
    "#"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
