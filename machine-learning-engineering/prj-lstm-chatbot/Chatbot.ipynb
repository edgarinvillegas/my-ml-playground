{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# ChatBot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM/GRU as it's memory unit.\n",
    "\n",
    "The dataset chosen is Squad v2 from `torchtext.datasets`. Check `prepare_data.py` for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code based on Matthew Inkawhich's <https://github.com/MatthewInkawhich>\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "import os\n",
    "from chatbot.prepare_data import load_prepare_data, trim_unfrequent_words, batch_to_train_data, MAX_SENTENCE_LENGTH, get_indexes_from_sentence, normalize_string\n",
    "from chatbot.models import Seq2Seq, SearchDecoder\n",
    "from chatbot.globals import SOS_TOKEN, device\n",
    "from torchtext.datasets import SQuAD2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data wrangling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Read 11873 sentence pairs\n",
      "Trimmed to 872 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 3389\n",
      "\n",
      "pairs:\n",
      "['were the normans in normandy', 'th and th centuries']\n",
      "['from countries did the norse originate', 'denmark iceland and norway']\n",
      "['was the duke in the battle of hastings', 'william the conqueror']\n",
      "['was dyrrachium located', 'the adriatic']\n",
      "['did emma marry', 'king ethelred ii']\n",
      "['was emma s brother', 'duke richard ii']\n",
      "['kicked ethelred out', 'sweyn forkbeard']\n",
      "['did harold ii die', 'battle of hastings']\n",
      "['killed harold ii', 'william ii']\n",
      "['was margaret s husband', 'king malcolm iii of scotland']\n",
      "keep_words 642 / 3386 = 0.1896\n",
      "Trimmed from 872 pairs to 25, 0.0287 of total\n",
      "input_variable: tensor([[ 46,  46, 186, 118,  72],\n",
      "        [  4,   4,  12,  10,  15],\n",
      "        [589, 101, 388, 513, 422],\n",
      "        [ 15, 399,  85, 314,  30],\n",
      "        [206,  97,   4,   5, 423],\n",
      "        [516,   5, 387,   2,   2],\n",
      "        [ 94, 632,   2,   0,   0],\n",
      "        [  2,   2,   0,   0,   0]])\n",
      "lengths: tensor([8, 8, 7, 6, 6])\n",
      "target_variable: tensor([[644, 399, 186, 514, 424],\n",
      "        [641,  91,   5,  15, 422],\n",
      "        [  2,  15,   4, 237,   2],\n",
      "        [  0, 617, 389, 470,   0],\n",
      "        [  0,   2,   2,   2,   0]])\n",
      "mask: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 0]], dtype=torch.uint8)\n",
      "max_target_len: 5\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SQuAD2(split='dev')\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"checkpoints\")\n",
    "vocab, pairs = load_prepare_data(train_dataset)\n",
    "\n",
    "# Print first 10 pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)\n",
    "\n",
    "MIN_COUNT = 3    # Min word count for trimming\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trim_unfrequent_words(vocab, pairs, MIN_COUNT)\n",
    "\n",
    "# Check sample data with a small batch size\n",
    "small_batch_size = 5\n",
    "batches = batch_to_train_data(vocab, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Model\n",
    "\n",
    "We define 3 main classes:\n",
    "- `Encoder`\n",
    "- `Decoder`\n",
    "- `Seq2Seq`\n",
    "Additionally, the `Attention` and `SearchDecoder` classes. And a loss function: `mask_NLLLoss`\n",
    "\n",
    "Check `chatbot/models.py`\n",
    "\n",
    "For both train or evaluate the model, we must initialize encoder and decoder (through Seq2Seq)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "hidden_size = 500\n",
    "batch_size = 64\n",
    "\n",
    "# Set checkpoint to load from disk. None to start training from scratch\n",
    "checkpoint_iter = 4000\n",
    "checkpoint_filename = os.path.join(save_dir, '{}'.format(hidden_size), '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "checkpoint_filename = None      # Comment this to train from scratch, uncomment to load saved checkpoint\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if checkpoint_filename:\n",
    "    print('Loading checkpoint from ', checkpoint_filename)\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(checkpoint_filename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['encoder']\n",
    "    decoder_sd = checkpoint['decoder']\n",
    "    encoder_optimizer_sd = checkpoint['encoder_optimizer']\n",
    "    decoder_optimizer_sd = checkpoint['decoder_optimizer']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    vocab.__dict__ = checkpoint['vocab_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(vocab.num_words, hidden_size)\n",
    "if checkpoint_filename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "\n",
    "# Initialize Seq2seq (and encoder & decoder)\n",
    "seq2seq = Seq2Seq(hidden_size, hidden_size, vocab.num_words, embedding)\n",
    "encoder = seq2seq.encoder\n",
    "decoder = seq2seq.decoder\n",
    "\n",
    "if checkpoint_filename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Ready!')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "We have `train_step` for a single iteration and `train_loop` which runs several epochs. The latter also saves checkpoints every `save_every` iteration in the `checkpoints` folder."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def train_step(input_variable, lengths, target_variable, mask, max_target_len, seq2seq, encoder_optimizer,\n",
    "               decoder_optimizer, batch_size, clip):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss, n_totals, print_losses = seq2seq.forward_batch(input_variable, target_variable, max_target_len, lengths, mask,\n",
    "                                                   teacher_forcing_ratio, batch_size)\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients in place\n",
    "    _ = nn.utils.clip_grad_norm_(seq2seq.encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(seq2seq.decoder.parameters(), clip)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    avg_loss = sum(print_losses) / n_totals\n",
    "    return avg_loss\n",
    "\n",
    "def train_loop(vocab, pairs, seq2seq, encoder_optimizer, decoder_optimizer, embedding, save_dir, epochs,\n",
    "               batch_size, print_every, save_every, clip, checkpoint_filename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch_to_train_data(vocab, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                        for _ in range(epochs)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if checkpoint_filename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for epoch in range(start_iteration, epochs + 1):\n",
    "        training_batch = training_batches[epoch - 1]\n",
    "        # Extract from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train_step(input_variable, lengths, target_variable, mask, max_target_len, seq2seq, encoder_optimizer,\n",
    "                          decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Epoch: {}; Completion: {:.1f}%; Avg loss: {:.4f}\".format(epoch, epoch / epochs * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (epoch % save_every == 0):\n",
    "            directory = os.path.join(save_dir, '{}'.format(hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': epoch,\n",
    "                'vocab_dict': vocab.__dict__,\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'encoder_optimizer': encoder_optimizer.state_dict(),\n",
    "                'decoder': decoder.state_dict(),\n",
    "                'decoder_optimizer': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(epoch, 'checkpoint')))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Initializing ...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\edgarin\\mycode\\my-ml-playground\\machine-learning-engineering\\prj-lstm-chatbot\\chatbot\\models.py:145: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:1856.)\n",
      "  loss = cross_entropy.masked_select(mask).mean()\n",
      "C:\\Users\\edgar\\anaconda3\\envs\\dlnd2-env\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:2280.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Completion: 2.5%; Avg loss: 6.4762\n",
      "Epoch: 2; Completion: 5.0%; Avg loss: 6.3502\n",
      "Epoch: 3; Completion: 7.5%; Avg loss: 6.1476\n",
      "Epoch: 4; Completion: 10.0%; Avg loss: 5.7190\n",
      "Epoch: 5; Completion: 12.5%; Avg loss: 5.6533\n",
      "Epoch: 6; Completion: 15.0%; Avg loss: 5.2770\n",
      "Epoch: 7; Completion: 17.5%; Avg loss: 5.0650\n",
      "Epoch: 8; Completion: 20.0%; Avg loss: 4.6833\n",
      "Epoch: 9; Completion: 22.5%; Avg loss: 4.5040\n",
      "Epoch: 10; Completion: 25.0%; Avg loss: 4.4756\n",
      "Epoch: 11; Completion: 27.5%; Avg loss: 4.0854\n",
      "Epoch: 12; Completion: 30.0%; Avg loss: 3.8745\n",
      "Epoch: 13; Completion: 32.5%; Avg loss: 3.6492\n",
      "Epoch: 14; Completion: 35.0%; Avg loss: 3.6343\n",
      "Epoch: 15; Completion: 37.5%; Avg loss: 3.5598\n",
      "Epoch: 16; Completion: 40.0%; Avg loss: 3.3270\n",
      "Epoch: 17; Completion: 42.5%; Avg loss: 3.4147\n",
      "Epoch: 18; Completion: 45.0%; Avg loss: 3.2389\n",
      "Epoch: 19; Completion: 47.5%; Avg loss: 3.2522\n",
      "Epoch: 20; Completion: 50.0%; Avg loss: 3.1396\n",
      "Epoch: 21; Completion: 52.5%; Avg loss: 3.0715\n",
      "Epoch: 22; Completion: 55.0%; Avg loss: 3.0769\n",
      "Epoch: 23; Completion: 57.5%; Avg loss: 3.0292\n",
      "Epoch: 24; Completion: 60.0%; Avg loss: 3.0061\n",
      "Epoch: 25; Completion: 62.5%; Avg loss: 3.0130\n",
      "Epoch: 26; Completion: 65.0%; Avg loss: 2.9043\n",
      "Epoch: 27; Completion: 67.5%; Avg loss: 2.8047\n",
      "Epoch: 28; Completion: 70.0%; Avg loss: 2.7224\n",
      "Epoch: 29; Completion: 72.5%; Avg loss: 2.6479\n",
      "Epoch: 30; Completion: 75.0%; Avg loss: 2.6629\n",
      "Epoch: 31; Completion: 77.5%; Avg loss: 2.5024\n",
      "Epoch: 32; Completion: 80.0%; Avg loss: 2.6069\n",
      "Epoch: 33; Completion: 82.5%; Avg loss: 2.5393\n",
      "Epoch: 34; Completion: 85.0%; Avg loss: 2.4260\n",
      "Epoch: 35; Completion: 87.5%; Avg loss: 2.3680\n",
      "Epoch: 36; Completion: 90.0%; Avg loss: 2.2997\n",
      "Epoch: 37; Completion: 92.5%; Avg loss: 2.2667\n",
      "Epoch: 38; Completion: 95.0%; Avg loss: 2.2388\n",
      "Epoch: 39; Completion: 97.5%; Avg loss: 1.9986\n",
      "Epoch: 40; Completion: 100.0%; Avg loss: 2.2333\n"
     ]
    }
   ],
   "source": [
    "# Configure training/optimization hyperparams.\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "epochs = 4000\n",
    "\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if checkpoint_filename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Start training...\")\n",
    "train_loop(vocab, pairs, seq2seq, encoder_optimizer, decoder_optimizer, embedding, save_dir, epochs, batch_size,\n",
    "           print_every, save_every, clip, checkpoint_filename)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot> the of\n",
      "Chatbot> the of\n",
      "Chatbot> the of\n",
      "ERROR: Unknown word.\n",
      "ERROR: Unknown word.\n"
     ]
    }
   ],
   "source": [
    "# Manages the low-level process of handling the input sentence\n",
    "def evaluate(searcher, vocab, sentence, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [get_indexes_from_sentence(vocab, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_sentence_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [vocab.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def eval_input(searcher, vocab):\n",
    "    while(True):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('You> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(searcher, vocab, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Chatbot>', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"ERROR: Unknown word.\")\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Run Evaluation\n",
    "# ~~~~~~~~~~~~~~\n",
    "#\n",
    "# To chat with your model, run the following block.\n",
    "#\n",
    "\n",
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = SearchDecoder(seq2seq)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "eval_input(searcher, vocab)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
